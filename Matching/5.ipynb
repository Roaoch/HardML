{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "09b56051-55dc-4337-bd4c-2e4a3324d709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.8/site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from torch) (1.19.2)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.8/site-packages (3.5)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.8/site-packages (from nltk) (2022.10.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "451dba73-0ed0-4efd-bddc-607f08ce641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "54d172e2-1df0-495c-8bf4-1d5050db3819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Tuple, Union, Callable\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Замените пути до директорий и файлов! Можете использовать для локальной отладки.\n",
    "# При проверке на сервере пути будут изменены\n",
    "glue_qqp_dir = './data/QQP'\n",
    "glove_path = './data/glove.6B.50d.txt'\n",
    "\n",
    "class GaussianKernel(torch.nn.Module):\n",
    "    def __init__(self, mu: float = 1., sigma: float = 1.):\n",
    "        super().__init__()\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.exp(\n",
    "            -torch.pow(x - self.mu, 2) / (2 * self.sigma**2)\n",
    "        )\n",
    "\n",
    "class KNRM(torch.nn.Module):\n",
    "    def __init__(self, embedding_matrix: np.ndarray, freeze_embeddings: bool, kernel_num: int = 21,\n",
    "                 sigma: float = 0.1, exact_sigma: float = 0.001,\n",
    "                 out_layers: List[int] = [10, 5]):\n",
    "        super().__init__()\n",
    "        self.embeddings = torch.nn.Embedding.from_pretrained(\n",
    "            torch.FloatTensor(embedding_matrix),\n",
    "            freeze=freeze_embeddings,\n",
    "            padding_idx=0\n",
    "        )\n",
    "\n",
    "        self.kernel_num = kernel_num\n",
    "        self.sigma = sigma\n",
    "        self.exact_sigma = exact_sigma\n",
    "        self.out_layers = out_layers\n",
    "\n",
    "        self.kernels = self._get_kernels_layers()\n",
    "\n",
    "        self.mlp = self._get_mlp()\n",
    "\n",
    "        self.out_activation = torch.nn.Sigmoid()\n",
    "\n",
    "    def _get_kernels_layers(self) -> torch.nn.ModuleList:\n",
    "        kernels = torch.nn.ModuleList()\n",
    "        threshold = 2 / (self.kernel_num - 1)\n",
    "        global_m = -1\n",
    "        for k in range(self.kernel_num):\n",
    "            if k == self.kernel_num - 1:\n",
    "                kernels.append(GaussianKernel(mu= 1, sigma=self.exact_sigma))\n",
    "            else:\n",
    "                global_m += threshold / 2 if k == 0 else threshold\n",
    "                kernels.append(GaussianKernel(mu=global_m, sigma=self.sigma))\n",
    "        return kernels\n",
    "\n",
    "    def _get_mlp(self) -> torch.nn.Sequential:\n",
    "        module_list = []\n",
    "        prev_neurons = len(self.kernels)\n",
    "        for rel in self.out_layers:\n",
    "            module_list.append(torch.nn.ReLU())\n",
    "            module_list.append(torch.nn.Linear(prev_neurons, rel))\n",
    "            prev_neurons = rel\n",
    "        if len(self.out_layers) != 0:\n",
    "            module_list.append(torch.nn.ReLU())\n",
    "        module_list.append(torch.nn.Linear(prev_neurons, 1))\n",
    "        \n",
    "        seq = torch.nn.Sequential(*module_list)\n",
    "        return seq\n",
    "\n",
    "    def forward(self, input_1: Dict[str, torch.Tensor], input_2: Dict[str, torch.Tensor]) -> torch.FloatTensor:\n",
    "        logits_1 = self.predict(input_1)\n",
    "        logits_2 = self.predict(input_2)\n",
    "\n",
    "        logits_diff = logits_1 - logits_2\n",
    "\n",
    "        out = self.out_activation(logits_diff)\n",
    "        return out\n",
    "\n",
    "    def _get_matching_matrix(self, query: torch.Tensor, doc: torch.Tensor) -> torch.FloatTensor:\n",
    "        query = self.embeddings(query)\n",
    "        doc = self.embeddings(doc)\n",
    "        query_norm = query / (query.norm(p=2, dim=-1, keepdim=True) + 1e-13)\n",
    "        doc_norm = doc / (doc.norm(p=2, dim=-1, keepdim=True) + 1e-13)\n",
    "        return torch.bmm(query_norm, doc_norm.transpose(-1, -2))\n",
    "\n",
    "    def _apply_kernels(self, matching_matrix: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        KM = []\n",
    "        for kernel in self.kernels:\n",
    "            # shape = [B]\n",
    "            K = torch.log1p(kernel(matching_matrix).sum(dim=-1)).sum(dim=-1)\n",
    "            KM.append(K)\n",
    "\n",
    "        # shape = [B, K]\n",
    "        kernels_out = torch.stack(KM, dim=1)\n",
    "        return kernels_out\n",
    "\n",
    "    def predict(self, inputs: Dict[str, torch.Tensor]) -> torch.FloatTensor:\n",
    "        # shape = [Batch, Left], [Batch, Right]\n",
    "        query, doc = inputs['query'], inputs['document']\n",
    "\n",
    "        # shape = [Batch, Left, Right]\n",
    "        matching_matrix = self._get_matching_matrix(query, doc)\n",
    "        \n",
    "        # shape = [Batch, Kernels]\n",
    "        kernels_out = self._apply_kernels(matching_matrix)\n",
    "        \n",
    "        # shape = [Batch]\n",
    "        out = self.mlp(kernels_out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class RankingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, index_pairs_or_triplets: List[List[Union[str, float]]],\n",
    "                 idx_to_text_mapping: Dict[str, str], vocab: Dict[str, int], oov_val: int,\n",
    "                 preproc_func: Callable, max_len: int = 30):\n",
    "        self.index_pairs_or_triplets = index_pairs_or_triplets\n",
    "        self.idx_to_text_mapping = idx_to_text_mapping\n",
    "        self.vocab = vocab\n",
    "        self.oov_val = oov_val\n",
    "        self.preproc_func = preproc_func\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_pairs_or_triplets)\n",
    "\n",
    "    def _tokenized_text_to_index(self, tokenized_text: List[str]) -> List[int]:\n",
    "        res = []\n",
    "        for token in tokenized_text:\n",
    "            if token in self.vocab:\n",
    "                res.append(self.vocab[token])\n",
    "            else:\n",
    "                res.append(self.vocab[\"OOV\"])\n",
    "        return res\n",
    "        \n",
    "    def _convert_text_idx_to_token_idxs(self, idx: int) -> List[int]:\n",
    "        text = self.preproc_func(self.idx_to_text_mapping[idx])\n",
    "        text = text[:self.max_len]\n",
    "        return self._tokenized_text_to_index(text)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        pass\n",
    "\n",
    "\n",
    "class TrainTripletsDataset(RankingDataset):\n",
    "    def __getitem__(self, idx) -> Tuple[Dict[str, List[int]], Dict[str, List[int]], int]:\n",
    "        triplet = self.index_pairs_or_triplets[idx]\n",
    "        return (\n",
    "            {\n",
    "                \"query\": self._convert_text_idx_to_token_idxs(triplet[0]),\n",
    "                \"document\": self._convert_text_idx_to_token_idxs(triplet[1]),\n",
    "            },\n",
    "            {\n",
    "                \"query\": self._convert_text_idx_to_token_idxs(triplet[0]),\n",
    "                \"document\": self._convert_text_idx_to_token_idxs(triplet[2]),\n",
    "            },\n",
    "            float(triplet[3])\n",
    "        )\n",
    "\n",
    "\n",
    "class ValPairsDataset(RankingDataset):\n",
    "    def __getitem__(self, idx) -> Tuple[Dict[str, torch.Tensor], int]:\n",
    "        pair = self.index_pairs_or_triplets[idx]\n",
    "        return ({\n",
    "            \"query\": self._convert_text_idx_to_token_idxs(pair[0]),\n",
    "            \"document\": self._convert_text_idx_to_token_idxs(pair[1]),\n",
    "        }, float(pair[2]))\n",
    "\n",
    "\n",
    "def collate_fn(batch_objs: List[Union[Dict[str, torch.Tensor], torch.FloatTensor]]):\n",
    "    max_len_q1 = -1\n",
    "    max_len_d1 = -1\n",
    "    max_len_q2 = -1\n",
    "    max_len_d2 = -1\n",
    "\n",
    "    is_triplets = False\n",
    "    for elem in batch_objs:\n",
    "        if len(elem) == 3:\n",
    "            left_elem, right_elem, label = elem\n",
    "            is_triplets = True\n",
    "        else:\n",
    "            left_elem, label = elem\n",
    "\n",
    "        max_len_q1 = max(len(left_elem['query']), max_len_q1)\n",
    "        max_len_d1 = max(len(left_elem['document']), max_len_d1)\n",
    "        if len(elem) == 3:\n",
    "            max_len_q2 = max(len(right_elem['query']), max_len_q2)\n",
    "            max_len_d2 = max(len(right_elem['document']), max_len_d2)\n",
    "\n",
    "    q1s = []\n",
    "    d1s = []\n",
    "    q2s = []\n",
    "    d2s = []\n",
    "    labels = []\n",
    "\n",
    "    for elem in batch_objs:\n",
    "        if is_triplets:\n",
    "            left_elem, right_elem, label = elem\n",
    "        else:\n",
    "            left_elem, label = elem\n",
    "\n",
    "        pad_len1 = max_len_q1 - len(left_elem['query'])\n",
    "        pad_len2 = max_len_d1 - len(left_elem['document'])\n",
    "        if is_triplets:\n",
    "            pad_len3 = max_len_q2 - len(right_elem['query'])\n",
    "            pad_len4 = max_len_d2 - len(right_elem['document'])\n",
    "\n",
    "        q1s.append(left_elem['query'] + [0] * pad_len1)\n",
    "        d1s.append(left_elem['document'] + [0] * pad_len2)\n",
    "        if is_triplets:\n",
    "            q2s.append(right_elem['query'] + [0] * pad_len3)\n",
    "            d2s.append(right_elem['document'] + [0] * pad_len4)\n",
    "        labels.append([label])\n",
    "    q1s = torch.LongTensor(q1s)\n",
    "    d1s = torch.LongTensor(d1s)\n",
    "    if is_triplets:\n",
    "        q2s = torch.LongTensor(q2s)\n",
    "        d2s = torch.LongTensor(d2s)\n",
    "    labels = torch.FloatTensor(labels)\n",
    "\n",
    "    ret_left = {'query': q1s, 'document': d1s}\n",
    "    if is_triplets:\n",
    "        ret_right = {'query': q2s, 'document': d2s}\n",
    "        return ret_left, ret_right, labels\n",
    "    else:\n",
    "        return ret_left, labels\n",
    "\n",
    "\n",
    "class Solution:\n",
    "    def __init__(self, glue_qqp_dir: str, glove_vectors_path: str,\n",
    "                 min_token_occurancies: int = 1,\n",
    "                 random_seed: int = 0,\n",
    "                 emb_rand_uni_bound: float = 0.2,\n",
    "                 freeze_knrm_embeddings: bool = True,\n",
    "                 knrm_kernel_num: int = 21,\n",
    "                 knrm_out_mlp: List[int] = [10, 5],\n",
    "                 dataloader_bs: int = 1024,\n",
    "                 train_lr: float = 0.001,\n",
    "                 change_train_loader_ep: int = 10\n",
    "                 ):\n",
    "        self.glue_qqp_dir = glue_qqp_dir\n",
    "        self.glove_vectors_path = glove_vectors_path\n",
    "        self.glue_train_df = self.get_glue_df('train')\n",
    "        self.glue_dev_df = self.get_glue_df('dev')\n",
    "        self.max_train_length = 8000\n",
    "        self.dev_pairs_for_ndcg = self.create_val_pairs(self.glue_dev_df)\n",
    "        self.triplets_for_train = self.sample_data_for_train_iter(self.glue_train_df, random_seed)\n",
    "        self.min_token_occurancies = min_token_occurancies\n",
    "        self.all_tokens = self.get_all_tokens(\n",
    "            [self.glue_train_df, self.glue_dev_df], self.min_token_occurancies)\n",
    "\n",
    "        self.random_seed = random_seed\n",
    "        self.emb_rand_uni_bound = emb_rand_uni_bound\n",
    "        self.freeze_knrm_embeddings = freeze_knrm_embeddings\n",
    "        self.knrm_kernel_num = knrm_kernel_num\n",
    "        self.knrm_out_mlp = knrm_out_mlp\n",
    "        self.dataloader_bs = dataloader_bs\n",
    "        self.train_lr = train_lr\n",
    "        self.change_train_loader_ep = change_train_loader_ep\n",
    "\n",
    "        self.model, self.vocab, self.unk_words = self.build_knrm_model()\n",
    "        self.idx_to_text_mapping_train = self.get_idx_to_text_mapping(\n",
    "            self.glue_train_df)\n",
    "        self.idx_to_text_mapping_dev = self.get_idx_to_text_mapping(\n",
    "            self.glue_dev_df)\n",
    "\n",
    "        self.val_dataset = ValPairsDataset(self.dev_pairs_for_ndcg,\n",
    "              self.idx_to_text_mapping_dev,\n",
    "              vocab=self.vocab, oov_val=self.vocab['OOV'],\n",
    "              preproc_func=self.simple_preproc)\n",
    "        self.val_dataloader = torch.utils.data.DataLoader(\n",
    "            self.val_dataset, batch_size=self.dataloader_bs, num_workers=0,\n",
    "            collate_fn=collate_fn, shuffle=False)\n",
    "        \n",
    "        self.train_dataset = TrainTripletsDataset(self._get_random_part_of_data(self.triplets_for_train),\n",
    "              self.idx_to_text_mapping_train,\n",
    "              vocab=self.vocab, oov_val=self.vocab['OOV'],\n",
    "              preproc_func=self.simple_preproc)\n",
    "        self.train_dataloader = torch.utils.data.DataLoader(\n",
    "            self.train_dataset, batch_size=self.dataloader_bs, num_workers=0,\n",
    "            collate_fn=collate_fn)\n",
    "\n",
    "    def get_glue_df(self, partition_type: str) -> pd.DataFrame:\n",
    "        assert partition_type in ['dev', 'train']\n",
    "        glue_df = pd.read_csv(\n",
    "            self.glue_qqp_dir + f'/{partition_type}.tsv', sep='\\t', error_bad_lines=False, dtype=object)\n",
    "        glue_df = glue_df.dropna(axis=0, how='any').reset_index(drop=True)\n",
    "        glue_df_fin = pd.DataFrame({\n",
    "            'id_left': glue_df['qid1'],\n",
    "            'id_right': glue_df['qid2'],\n",
    "            'text_left': glue_df['question1'],\n",
    "            'text_right': glue_df['question2'],\n",
    "            'label': glue_df['is_duplicate'].astype(int)\n",
    "        })\n",
    "        return glue_df_fin\n",
    "\n",
    "    def hadle_punctuation(self, inp_str: str) -> str:\n",
    "        #return re.sub(f\"[{string.punctuation}]\", \"PUNCT_TOKEN\", inp_str)\n",
    "        inp_str = inp_str.replace(\"\\\\\", \"/\")\n",
    "        return re.sub(f\"[{string.punctuation}]\", \" \", inp_str)\n",
    "\n",
    "    def simple_preproc(self, inp_str: str) -> List[str]:\n",
    "        res = nltk.word_tokenize(self.hadle_punctuation(inp_str.lower()))\n",
    "        return res\n",
    "\n",
    "    def _filter_rare_words(self, vocab: Dict[str, int], min_occurancies: int) -> Dict[str, int]:\n",
    "        return dict(filter(lambda item: item[1] >= min_occurancies, vocab.items()))\n",
    "\n",
    "    def get_all_tokens(self, list_of_df: List[pd.DataFrame], min_occurancies: int) -> List[str]:        \n",
    "        vocab = {}\n",
    "        all_texts = []\n",
    "        for df in list_of_df:\n",
    "            for row in df.values:\n",
    "                all_texts.append(row[2])\n",
    "                all_texts.append(row[3])\n",
    "        uniq_text = set(all_texts)\n",
    "        for text in uniq_text:\n",
    "            for single_token in self.simple_preproc(text):\n",
    "                if single_token in vocab:\n",
    "                    vocab[single_token] += 1\n",
    "                else:\n",
    "                    vocab.update({single_token: 1})\n",
    "        return list(self._filter_rare_words(vocab, min_occurancies).keys())\n",
    "\n",
    "    def _read_glove_embeddings(self, file_path: str) -> Dict[str, List[str]]:\n",
    "        embeddings_dict = {}\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                vector = np.asarray(values[1:], \"float32\")\n",
    "                embeddings_dict[word] = vector\n",
    "        return embeddings_dict\n",
    "\n",
    "    def create_glove_emb_from_file(self, file_path: str, inner_keys: List[str],\n",
    "                                   random_seed: int, rand_uni_bound: float\n",
    "                                   ) -> Tuple[np.ndarray, Dict[str, int], List[str]]:\n",
    "        pretrained_embeddings = self._read_glove_embeddings(file_path)\n",
    "        matrix_x = len(self.all_tokens) + 2\n",
    "        matrix_y = len(next(iter(pretrained_embeddings.values())))\n",
    "        matrix = np.ndarray(shape=(matrix_x, matrix_y), dtype=\"float\")\n",
    "        \n",
    "        vocab = {\n",
    "            \"PAD\": 0,\n",
    "            \"OOV\": 1\n",
    "        }\n",
    "        unk_words = [\"PAD\", \"OOV\"]\n",
    "        \n",
    "        matrix[0] = np.zeros((1, matrix_y))\n",
    "        matrix[1] = np.random.uniform(low=-0.2, high=0.2, size=(1, matrix_y))\n",
    "        for i, token in enumerate(self.all_tokens, start=2):\n",
    "            if token in pretrained_embeddings:\n",
    "                matrix[i] = pretrained_embeddings[token]\n",
    "            else:\n",
    "                unk_words.append(token)\n",
    "                matrix[i] = np.random.uniform(low=-0.2, high=0.2, size=(1, matrix_y))\n",
    "            vocab.update({token: i})\n",
    "        \n",
    "        return (matrix, vocab, unk_words)\n",
    "\n",
    "    def build_knrm_model(self) -> Tuple[torch.nn.Module, Dict[str, int], List[str]]:\n",
    "        emb_matrix, vocab, unk_words = self.create_glove_emb_from_file(\n",
    "            self.glove_vectors_path, self.all_tokens, self.random_seed, self.emb_rand_uni_bound)\n",
    "        torch.manual_seed(self.random_seed)\n",
    "        knrm = KNRM(emb_matrix, freeze_embeddings=self.freeze_knrm_embeddings,\n",
    "                    out_layers=self.knrm_out_mlp, kernel_num=self.knrm_kernel_num)\n",
    "        return knrm, vocab, unk_words\n",
    "    \n",
    "    def _get_random_part_of_data(self, data: List[List[Union[str, float]]]) -> torch.Tensor:\n",
    "        max_len = len(self.triplets_for_train)\n",
    "        to_nd = np.array(data)\n",
    "        rand_indx =  np.random.randint(low=0, high=max_len - 1, size=(self.max_train_length))\n",
    "        return to_nd[rand_indx]\n",
    "    \n",
    "    def sample_data_for_train_iter(self, inp_df: pd.DataFrame, seed: int\n",
    "                                   ) -> List[List[Union[str, float]]]:\n",
    "        inp_df_select = inp_df[['id_left', 'id_right', 'label']]\n",
    "        inf_df_group_sizes = inp_df_select.groupby('id_left').size()\n",
    "        glue_dev_leftids_to_use = list(\n",
    "            inf_df_group_sizes[inf_df_group_sizes >= 3].index\n",
    "        )\n",
    "        groups = inp_df_select[inp_df_select.id_left.isin(\n",
    "            glue_dev_leftids_to_use\n",
    "        )].groupby('id_left')\n",
    "        all_ids = inp_df_select['id_left'].values\n",
    "        out_triplets = []\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        for id_left, group in groups:\n",
    "            group[[\"label\"]] += 1\n",
    "            all_relevant = group.id_right.values\n",
    "            rand_ids = []\n",
    "            for _ in range(len(all_relevant)):\n",
    "                current_rand = np.random.choice(all_ids)\n",
    "                while current_rand in all_relevant:\n",
    "                    current_rand = np.random.choice(all_ids)\n",
    "                rand_ids.append([id_left, current_rand, 0])\n",
    "            group = group.append(pd.DataFrame(rand_ids, columns=[\"id_left\", \"id_right\", \"label\"]))\n",
    "            for top in group.values:\n",
    "                for bottom in group[group.label != top[2]].values:\n",
    "                    difference = top[2] - bottom[2]\n",
    "                    out_triplets.append([\n",
    "                        id_left, \n",
    "                        top[1], \n",
    "                        bottom[1], \n",
    "                        math.exp(difference) / (1 + math.exp(difference))\n",
    "                    ])\n",
    "        return out_triplets\n",
    " \n",
    "    def create_val_pairs(self, inp_df: pd.DataFrame, fill_top_to: int = 15,\n",
    "                         min_group_size: int = 2, seed: int = 0) -> List[List[Union[str, float]]]:\n",
    "        inp_df_select = inp_df[['id_left', 'id_right', 'label']]\n",
    "        inf_df_group_sizes = inp_df_select.groupby('id_left').size()\n",
    "        glue_dev_leftids_to_use = list(\n",
    "            inf_df_group_sizes[inf_df_group_sizes >= min_group_size].index\n",
    "        )\n",
    "        groups = inp_df_select[inp_df_select.id_left.isin(\n",
    "            glue_dev_leftids_to_use\n",
    "        )].groupby('id_left')\n",
    "\n",
    "        all_ids = set(inp_df['id_left']).union(set(inp_df['id_right']))\n",
    "\n",
    "        out_pairs = []\n",
    "\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        for id_left, group in groups:\n",
    "            ones_ids = group[group.label > 0].id_right.values\n",
    "            zeroes_ids = group[group.label == 0].id_right.values\n",
    "            sum_len = len(ones_ids) + len(zeroes_ids)\n",
    "            num_pad_items = max(0, fill_top_to - sum_len)\n",
    "            if num_pad_items > 0:\n",
    "                cur_chosen = set(ones_ids).union(\n",
    "                    set(zeroes_ids)).union({id_left})\n",
    "                pad_sample = np.random.choice(\n",
    "                    list(all_ids - cur_chosen), num_pad_items, replace=False).tolist()\n",
    "            else:\n",
    "                pad_sample = []\n",
    "            for i in ones_ids:\n",
    "                out_pairs.append([id_left, i, 2])\n",
    "            for i in zeroes_ids:\n",
    "                out_pairs.append([id_left, i, 1])\n",
    "            for i in pad_sample:\n",
    "                out_pairs.append([id_left, i, 0])\n",
    "        return out_pairs\n",
    "\n",
    "    def get_idx_to_text_mapping(self, inp_df: pd.DataFrame) -> Dict[str, str]:\n",
    "        left_dict = (\n",
    "            inp_df\n",
    "            [['id_left', 'text_left']]\n",
    "            .drop_duplicates()\n",
    "            .set_index('id_left')\n",
    "            ['text_left']\n",
    "            .to_dict()\n",
    "        )\n",
    "        right_dict = (\n",
    "            inp_df\n",
    "            [['id_right', 'text_right']]\n",
    "            .drop_duplicates()\n",
    "            .set_index('id_right')\n",
    "            ['text_right']\n",
    "            .to_dict()\n",
    "        )\n",
    "        left_dict.update(right_dict)\n",
    "        return left_dict\n",
    "\n",
    "    def ndcg_k(self, ys_true: np.array, ys_pred: np.array, ndcg_top_k: int = 10) -> float:\n",
    "        def _compute_gain(y_value: float) -> float:\n",
    "            return 2 ** y_value - 1\n",
    "        def _dcg_k(ys_true: torch.Tensor, ys_pred: torch.Tensor, k: int) -> float:\n",
    "            _, indices = torch.sort(ys_pred, descending=True)\n",
    "            sorted_true = ys_true[indices][:k].numpy()\n",
    "            gain = _compute_gain(sorted_true)\n",
    "            discount = [math.log2(float(x)) for x in range(2, len(sorted_true) + 2)]\n",
    "            discounted_gain = float((gain / discount).sum())\n",
    "            return discounted_gain\n",
    "        \n",
    "        current_dcg = _dcg_k(torch.Tensor(ys_true), torch.Tensor(ys_pred), ndcg_top_k)\n",
    "        ideal_dcg = _dcg_k(torch.Tensor(ys_true), torch.Tensor(ys_true), ndcg_top_k)\n",
    "        return current_dcg / ideal_dcg\n",
    "\n",
    "    def valid(self, model: torch.nn.Module, val_dataloader: torch.utils.data.DataLoader) -> float:\n",
    "        labels_and_groups = val_dataloader.dataset.index_pairs_or_triplets\n",
    "        labels_and_groups = pd.DataFrame(labels_and_groups, columns=['left_id', 'right_id', 'rel'])\n",
    "        \n",
    "        all_preds = []\n",
    "        for batch in (val_dataloader):\n",
    "            inp_1, y = batch\n",
    "            preds = model.predict(inp_1)\n",
    "            preds_np = preds.detach().numpy()\n",
    "            all_preds.append(preds_np)\n",
    "        all_preds = np.concatenate(all_preds, axis=0)\n",
    "        labels_and_groups['preds'] = all_preds\n",
    "\n",
    "        ndcgs = []\n",
    "        for cur_id in labels_and_groups.left_id.unique():\n",
    "            cur_df = labels_and_groups[labels_and_groups.left_id == cur_id]\n",
    "            ndcg = self.ndcg_k(cur_df.rel.values.reshape(-1), cur_df.preds.values.reshape(-1))\n",
    "            if np.isnan(ndcg):\n",
    "                ndcgs.append(0)\n",
    "            else:\n",
    "                ndcgs.append(ndcg)\n",
    "        return np.mean(ndcgs)\n",
    "\n",
    "    def train(self, n_epochs: int):\n",
    "        opt = torch.optim.SGD(self.model.parameters(), lr=self.train_lr)\n",
    "        criterion = torch.nn.BCELoss()\n",
    "        for epoch in range(n_epochs):\n",
    "            epoch_loss = 0\n",
    "            if ((epoch + 1) % 6 == 0):\n",
    "                opt.zero_grad()\n",
    "            if ((epoch + 1) % self.change_train_loader_ep == 0):\n",
    "                self.train_dataset = TrainTripletsDataset(self._get_random_part_of_data(self.triplets_for_train),\n",
    "                    self.idx_to_text_mapping_train,\n",
    "                    vocab=self.vocab, oov_val=self.vocab['OOV'],\n",
    "                    preproc_func=self.simple_preproc)\n",
    "                self.train_dataloader = torch.utils.data.DataLoader(\n",
    "                    self.train_dataset, batch_size=self.dataloader_bs, num_workers=0,\n",
    "                    collate_fn=collate_fn)\n",
    "            \n",
    "            for batch in (self.train_dataloader):\n",
    "                inp_top, inp_bottom, y = batch\n",
    "                p_pred = self.model(inp_top, inp_bottom)\n",
    "                loss = criterion(p_pred, y)\n",
    "                epoch_loss += loss.item()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "            # TEST\n",
    "            ndcg = self.valid(self.model, self.val_dataloader)\n",
    "            print(f\"Ndcg = {ndcg}, Loss(BCE) = {epoch_loss}\")\n",
    "            if ndcg > 0.925:\n",
    "                break\n",
    "            # PROD\n",
    "            # if epoch > 11:\n",
    "            #     ndcg = self.valid(self.model, self.val_dataloader)\n",
    "            #     if ndcg > 0.925:\n",
    "            #         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "aaa9fef0-0ead-4f6f-b1bb-9d5b9e9c99dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ndcg = 0.4221826026831567, Loss(BCE) = 6.364895343780518\n",
      "Ndcg = 0.5213643444545011, Loss(BCE) = 6.249792158603668\n",
      "Ndcg = 0.624031522020239, Loss(BCE) = 6.198362469673157\n",
      "Ndcg = 0.6353290568789983, Loss(BCE) = 6.171390652656555\n",
      "Ndcg = 0.670037795588321, Loss(BCE) = 6.1774203181266785\n",
      "Ndcg = 0.6758872224692801, Loss(BCE) = 6.102036952972412\n",
      "Ndcg = 0.6743253271959468, Loss(BCE) = 6.062164127826691\n",
      "Ndcg = 0.6619873531110049, Loss(BCE) = 6.04986971616745\n",
      "Ndcg = 0.6452187499002836, Loss(BCE) = 6.009912967681885\n",
      "Ndcg = 0.6513971351647659, Loss(BCE) = 6.013763904571533\n",
      "Ndcg = 0.6642630645641389, Loss(BCE) = 5.9899197816848755\n",
      "Ndcg = 0.6738608380539134, Loss(BCE) = 5.9610995054244995\n",
      "Ndcg = 0.6749994110883608, Loss(BCE) = 5.9521947503089905\n",
      "Ndcg = 0.6812295791197736, Loss(BCE) = 5.93231600522995\n",
      "Ndcg = 0.6924395754996449, Loss(BCE) = 5.905625820159912\n",
      "Ndcg = 0.6979372952164775, Loss(BCE) = 5.91767954826355\n",
      "Ndcg = 0.7181895845528807, Loss(BCE) = 5.8810813426971436\n",
      "Ndcg = 0.7353698511117714, Loss(BCE) = 5.865846216678619\n",
      "Ndcg = 0.7356622736286923, Loss(BCE) = 5.842407405376434\n",
      "Ndcg = 0.7401610376522457, Loss(BCE) = 5.871477782726288\n"
     ]
    }
   ],
   "source": [
    "sol = Solution(glue_qqp_dir, glove_path)\n",
    "sol.train(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4510f0a5-6032-42a2-8b07-a04c1b5a4ca8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
